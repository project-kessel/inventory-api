# Consumer Message Processing Failures

## Prerequisites

Remediations covered in this guide will require the following:
1. Kessel Debug container
2. [zed cli](https://github.com/authzed/zed?tab=readme-ov-file#getting-started) installed locally for some steps
2. Access to running Inventory API pod logs

### Kessel Debug Container

The Kessel Debug container contains all the tools and endpoints pre-configured for fixing numerous issues, including connecting to SpiceDB with `zed` cli, and Kafka admin scripts. If you need to access either Kafka or SpiceDB, it's recommended to use the debug container

To use:

1. Spin up the Kessel Debug container

```bash
oc process --local \
    -f https://raw.githubusercontent.com/project-kessel/inventory-api/refs/heads/main/tools/kessel-debug-container/kessel-debug-deploy.yaml \
    -p ENV=stage | oc apply -f -
```

2. Access the Kessel Debug container and configure

```bash
oc rsh kessel-debug

# If you need to access Kafka -- Setup Kafka env vars
source /usr/local/bin/env-setup.sh
```

Zed is already configured for the target env using evironment variables since using the `context` command fails because you won't have permissions to write the to the config location due to pod security. You can validate access by running `zed schema read`

(For more on the Kessel Debug container, check out the [README](https://github.com/project-kessel/inventory-api/tree/main/tools/kessel-debug-container#running-the-debug-container))

When done, always make sure to remove the Kessel Debug container

```bash
# after exiting from your rsh session
oc process --local \
    -f https://raw.githubusercontent.com/project-kessel/inventory-api/refs/heads/main/tools/kessel-debug-container/kessel-debug-deploy.yaml \
    -p ENV=stage | oc apply -f -
```

## Schema Related Issues

There are two known distinct reasons tuple creation could fail due to schema issues:
1. The schema definitions between Inventory API and Relations API are not in sync
2. The schema is valid but the tuple request values are malformed by Inventory API

### Inventory Consumer fails to Create/Modify a Relationship due to schema mismatch

**Example Error in Inventory API Logs**

```
msg=request failed: error creating tuple: rpc error: code = FailedPrecondition desc = error creating tuples: error writing relationships to SpiceDB: rpc error: code = FailedPrecondition desc = object definition `notifications/host` not found
```

**Reason**

The error `object definition "OBJECT" not found` indicates the schema loaded by SpiceDB does not contain a defintion for the tuple request. This is likely due to a mismatch between Inventory API and Relations API schema definitions

**Verify and Remediate**

You can verify if there is a schema mismatch one of two ways:

1. Check the loaded schema in Relations API for the object from the Kessel Debug container

```shell
zed schema read | grep <OBJECT>

# using the example error above
zed schema read | grep "notifications/host"
```

If nothing is returned, the loaded schema is missing the expected definition

2. Compare the loaded schema to the expected schema locally

The schema is deployed via App Interface and is visible in the [`spicedb-schema`](https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/resources/insights-common/kessel/kessel-spicedb-schema-configmap.yml?ref_type=heads) ConfigMap in cluster. The ConfigMap is generated by pulling from Github using a specific commit hash. When comparing the loaded schema to the expected schema, the commit ref listed in App Interface should be used


```shell
# capture the commit ref in app interface
COMMIT_REF=<value-in-app-interface>

# capture the loaded schema in SpiceDB
TMPDIR=$(mktemp -d) && pushd $TMPDIR

oc port-forward svc/kessel-relations-spicedb 50051:50051

# use zed cli to capture the configured schema in SpiceDB
export ZED_ENDPOINT=localhost:50051
export ZED_INSECURE=true

zed schema read > loaded-schema.zed

# capture the schema def from the configmap
oc get cm spicedb-schema -o json | jq -r '.data."schema.zed"' > configmap.zed

# download the stage schema from git using the commit ref
curl -O https://raw.githubusercontent.com/RedHatInsights/rbac-config/${COMMIT_REF}/configs/stage/schemas/schema.zed

# compare the downloaded schema to configmap schema
zed schema diff schema.zed configmap.zed

# compare the downloaded schema to the loaded schema from SpiceDB
zed schema diff schema.zed loaded-schema.zed

# when the issue is wrapped up, you can clean up with
popd && rm -rf $TMPDIR
```

The output of the `zed schema diff` commands will have no output when they match. Any differences will be shown which indicates a schema mismatch

If the downloaded schema and configmap schema dont match:
* there is an issue with the deployed configmap and the configmap must be redeployed with the correct hash via App Interface

If the downloaded schema and configmap match but the loaded schema doesnt:
* SpiceDB has not properly loaded the configmap, restarting the Relations API pods should reload the configmap

If all schema definitions match and the expected object from the error log does not exist in any of them, its possible the schema defintion for Relations has not been updated yet for the resource, or its possible Inventory has created a malformed request. See [Inventory Consumer fails due to malformed Tuple request](#inventory-consumer-fails-due-to-malformed-tuple-request) on how to check Inventory schema definitions for issues.

### Inventory Consumer fails due to malformed Tuple request

**Example Error in Inventory API Logs**

```
msg=request failed: error creating tuple: rpc error: code = FailedPrecondition desc = error creating tuples: error writing relationships to SpiceDB: rpc error: code = FailedPrecondition desc = object definition `notifications/host` not found
```

**Reason**

The error `object definition "OBJECT" not found` indicates the schema loaded by SpiceDB does not contain a defintion for the tuple request. If after review of the schema's via the [above runbook](#inventory-consumer-fails-to-createmodify-a-relationship-due-to-schema-mismatch) confirms the schemas are properly updated, the issue may be a malformed tuple. Reviewing the Inventory schemas can confirm if the tuple type should be supported or not and whether there is a bug in Inventory code.

**Verify and Remediate**

Inventory resource schemas are defined for each resource type supported by Kessel. If the Inventory Consumer fails to create/modify tuples due to schema definition issues, and no issues are found with the Relations API schemas, its possible Inventory is somehow creating malformed tuple requests. A review of the schema definitions can confirm if a resource type and reporter type is supported.

The supported resource types can be found in the [schemas folder](https://github.com/project-kessel/inventory-api/tree/main/data/schema/resources)

Each resource contains a `config.yaml` file in the root of its directory that lists the supported `resource_type`'s and `reporter_name`'s

For example: The Kubernetes Cluster resource supports ACM, ACS, and OCM as reporter types [LINK](https://github.com/project-kessel/inventory-api/blob/main/data/schema/resources/k8s_cluster/config.yaml)

From the error above, `notifications/host` should reflect the format of `reporter_name/resource_type` where `reporter_type` can only be one of the listed `reporter_name`s defined in the schemas and `resource_type` correlates to the same value in the `config.yaml`

Using the same error example, the resource type `host` only supports the reporter name `hbi` and therefore `notifications/host` would not be a valid tuple. This means Inventory has created an invalid tuple and there is likely a bug.

Tuple requests are generated based on messages consumed by the Inventory Consumer. To restore services, update the Inventory Consumers offset to skip the bad message allowing it to continue to process any messages in the queue after it. See [Skipping Offsets to Restore Processing](#skipping-offsets-to-restore-processing)

### Skipping Offsets to Restore Processing

> [!WARNING]
> Use Caution when Skipping Messages in a topic. Skipping messages means a resource will not be created/updated/deleted in Kessel Inventory and could lead to consistency issues down the road. Ensure service providers are aware of the issue to ensure they perform schema and API validation prior to publishing events.

To restore services due to a message that cannot be processed, update the Inventory Consumers offset to skip the bad message allowing it to continue to process any messages in the queue after it. Capturing the event for the service provider is also recommended to show the exact issue and capture info about the resource if needed.

To Update the Offset:

1. Determine the topic that the failing message comes from; the topic name and offset information are available in the error logs

```bash
# Example Error Log
ERROR ts=2025-09-08T04:33:44Z caller=log/log.go:30 service.name=inventory-api service.version=0.1.0 trace.id= span.id= subsystem=inventoryConsumer msg=error processing message: topic=outbox.event.kessel.tuples partition=0 offset=10
```

2. Spin up the Kessel Debug container to connect to Kafka (if you haven't already -- see [Kessel Debug container](#kessel-debug-container) above)

3. Access the Kessel Debug container and configure Kafka

```bash
oc rsh kessel-debug

# Setup Kafka env vars
source /usr/local/bin/env-setup.sh
```

4. Capture the event message for the failing offset

```bash
# confirm current offset and lag -- the current offset should be close to, if not the message directly before the offset mentioned in logs
./bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS --command-config $KAFKA_AUTH_CONFIG --group inventory-consumer --describe

# Example Output
GROUP                   TOPIC                        PARTITION  CURRENT-OFFSET  LOG-END-OFFSET
inventory-consumer      outbox.event.kessel-tuples   0          9               50
```

In the above output, `CURRENT-OFFSET` indicates the last message offset processed, `LOG-END-OFFSET` captures that last offset that exists in the topic. Based on the current offset, the likely culprit is offset `10`, we can look at that message by consuming from the topic and searching for that offset

```bash
./bin/kafka-console-consumer.sh --topic outbox.event.kessel.tuples --bootstrap-server $KAFKA_CONNECT_BOOTSTRAP_SERVERS --from-beginning --property print.key=true --property print.headers=true --property print.offset=true | grep Offset:<OFFSET-NUMBER>
```

This will print out the event key, event headers, event offset number and the entire message including message schema. Capture all of this data for records in case any of it is pertinent to the Kessel team or the service provider affected.


5. Update the offset for the Inventory Consumer Group

```bash
# confirm current offset and lag -- the current offset should be close to, if not the message directly before the offset mentioned in logs
./bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS --command-config $KAFKA_AUTH_CONFIG --group inventory-consumer --describe

# shift to next offset to skip the bad message
./bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS --command-config $KAFKA_AUTH_CONFIG --group inventory-consumer --reset-offsets --shift-by 1 --execute --topic outbox.event.kessel.tuples
```

The Internal Inventory Consumer should move to the next message in the queue after completing and should continue processing as normal. Note, the `kafka-consumer-groups.sh` command generally expects the consumer to not be active in order to complete. It may be required to disable the consumer before shifting the offset. Consumer failures have a retry loop with backoff, executing the command in those waiting periods generally is sufficient. If the consumer needs to be disabled, it can be done through the Inventory API configmap:

```yaml
consumer:
  enabled: false
```

Once the message has been skipped, re-enable the consumer and make sure the pod is restarted and the consumer will continue processing. If the problem persists on subsequent messages, this process would need to be repeated until all failed messages have been skipped.

4. Clean Up

Once done, make sure to remove the Kessel Debug container

```bash
# after exiting from your rsh session
oc process --local \
    -f https://raw.githubusercontent.com/project-kessel/inventory-api/refs/heads/main/tools/kessel-debug-container/kessel-debug-deploy.yaml \
    -p ENV=stage | oc apply -f -
```
